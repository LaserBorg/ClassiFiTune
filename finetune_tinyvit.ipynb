{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a393ad",
   "metadata": {},
   "source": [
    "# TinyViT Training \n",
    "with Pretrained Weights and Two-Stage Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"libs.tiny_vit.tiny_vit\")\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import classification_report\n",
    "#from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "from libs.common import load_dict, dump_dict\n",
    "from libs.albumentations_utils import AlbumentationsTransform\n",
    "from libs.tiny_vit.tiny_vit_train import get_model, load_pretrained_weights, freeze_backbone, unfreeze_all, \\\n",
    "    set_weight_decay, get_cosine_scheduler_with_warmup, train_epoch, validate_epoch, plot_confusion_matrix, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ecd9ef",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b403d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = load_dict('settings_tinyvit.yaml')\n",
    "\n",
    "# Load base model configuration from tiny_vit.json\n",
    "base_model = settings['base_model']\n",
    "variants_path = \"libs/tiny_vit/tiny_vit.json\"\n",
    "variants = load_dict(variants_path)\n",
    "\n",
    "if base_model not in variants:\n",
    "    raise ValueError(f\"Base model '{base_model}' not found in available variants: {list(variants.keys())}\")\n",
    "\n",
    "base_model_info = variants[base_model]\n",
    "\n",
    "# Extract model configuration and paths from base model\n",
    "model_config = base_model_info['model_config']\n",
    "pretrained_path = base_model_info['weights']\n",
    "img_size = model_config['img_size']\n",
    "\n",
    "print(f\"Using base model: {base_model}\")\n",
    "print(f\"Image size: {img_size}x{img_size}\")\n",
    "print(f\"Pretrained weights: {pretrained_path}\")\n",
    "\n",
    "# Directory settings\n",
    "data_dir = settings['data_dir']\n",
    "output_dir = settings['output_dir']\n",
    "logs_dir = settings['logs_dir']\n",
    "\n",
    "# Output file names\n",
    "checkpoint_name = settings['checkpoint_name']\n",
    "\n",
    "# Derived directory paths\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Count classes\n",
    "num_classes = sum(os.path.isdir(os.path.join(train_dir, entry)) for entry in os.listdir(train_dir))\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "# Normalization settings\n",
    "mean = np.array(settings['mean'])  # np.array(IMAGENET_DEFAULT_MEAN)\n",
    "std = np.array(settings['std'])    # np.array(IMAGENET_DEFAULT_STD)\n",
    "\n",
    "# Stage 1 training parameters (Head training with frozen backbone)\n",
    "stage1_epochs = settings['stage1']['epochs']\n",
    "stage1_lr = float(settings['stage1']['learning_rate'])\n",
    "stage1_warmup_epochs = settings['stage1']['warmup_epochs']\n",
    "stage1_min_lr = float(settings['stage1']['min_lr'])\n",
    "stage1_batch_size = settings['stage1']['batch_size']\n",
    "\n",
    "# Stage 2 training parameters (Full fine-tuning)  \n",
    "stage2_epochs = settings['stage2']['epochs']\n",
    "stage2_lr = float(settings['stage2']['learning_rate'])\n",
    "stage2_warmup_epochs = settings['stage2']['warmup_epochs']\n",
    "stage2_min_lr = float(settings['stage2']['min_lr'])\n",
    "stage2_batch_size = settings['stage2']['batch_size']\n",
    "\n",
    "# Additional training settings\n",
    "layer_lr_decay = float(settings['layer_lr_decay'])\n",
    "weight_decay = float(settings['weight_decay'])\n",
    "use_amp = settings['use_amp']\n",
    "patience = settings['patience']\n",
    "gradient_clip_norm = float(settings['gradient_clip_norm'])\n",
    "eval_bn = settings['eval_bn']\n",
    "\n",
    "# Optimizer settings\n",
    "optimizer_config = settings['optimizer']\n",
    "optimizer_config['eps'] = float(optimizer_config['eps'])\n",
    "optimizer_config['betas'] = [float(b) for b in optimizer_config['betas']]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter(logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721dd17",
   "metadata": {},
   "source": [
    "## transforms and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get augmentation settings from config\n",
    "aug_config = settings['augmentation']\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.HorizontalFlip(p=aug_config['horizontal_flip']),\n",
    "    A.Rotate(limit=aug_config['rotation_limit'], p=aug_config['rotation_prob']),\n",
    "    A.ColorJitter(\n",
    "        brightness=aug_config['color_jitter']['brightness'], \n",
    "        contrast=aug_config['color_jitter']['contrast'], \n",
    "        saturation=aug_config['color_jitter']['saturation'], \n",
    "        hue=aug_config['color_jitter']['hue'], \n",
    "        p=aug_config['color_jitter']['prob']\n",
    "    ),\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(\n",
    "            var_limit=aug_config['noise_and_blur']['gaussian_noise']['var_limit'], \n",
    "            p=aug_config['noise_and_blur']['gaussian_noise']['prob']\n",
    "        ),\n",
    "        A.GaussianBlur(\n",
    "            blur_limit=aug_config['noise_and_blur']['gaussian_blur']['blur_limit'], \n",
    "            p=aug_config['noise_and_blur']['gaussian_blur']['prob']\n",
    "        ),\n",
    "    ], p=aug_config['noise_and_blur']['prob']),\n",
    "    A.CoarseDropout(\n",
    "        max_holes=aug_config['coarse_dropout']['max_holes'], \n",
    "        max_height=aug_config['coarse_dropout']['max_height'], \n",
    "        max_width=aug_config['coarse_dropout']['max_width'], \n",
    "        p=aug_config['coarse_dropout']['prob']\n",
    "    ),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "eval_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Datasets and Dataloaders\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    os.path.join(data_dir, \"train\"),\n",
    "    transform=AlbumentationsTransform(train_transform)\n",
    ")\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    os.path.join(data_dir, \"val\"),\n",
    "    transform=AlbumentationsTransform(eval_transform)\n",
    ")\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    os.path.join(data_dir, \"test\"),\n",
    "    transform=AlbumentationsTransform(eval_transform)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=stage1_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=stage1_batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=stage1_batch_size)\n",
    "\n",
    "# Get class names and create class labels mapping\n",
    "class_labels = train_dataset.classes\n",
    "print(f\"Classes found: {class_labels}\")\n",
    "\n",
    "# Build model using integrated model configuration\n",
    "model = get_model(model_config, num_classes, img_size, device)\n",
    "\n",
    "# Load pretrained weights\n",
    "model = load_pretrained_weights(model, pretrained_path, num_classes, device)\n",
    "\n",
    "# Initialize mixed precision scaler\n",
    "scaler = GradScaler('cuda') if use_amp else None\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241031a",
   "metadata": {},
   "source": [
    "## STAGE 1: Training classifier head with frozen backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e322397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze backbone and train only the head\n",
    "model = freeze_backbone(model)\n",
    "\n",
    "# Optimizer and scheduler for stage 1 with improved weight decay\n",
    "param_groups = set_weight_decay(model, weight_decay=weight_decay)\n",
    "optimizer = optim.AdamW(\n",
    "    param_groups, \n",
    "    lr=stage1_lr, \n",
    "    eps=optimizer_config['eps'], \n",
    "    betas=optimizer_config['betas']\n",
    ")\n",
    "\n",
    "# Cosine annealing scheduler with warmup\n",
    "total_steps = stage1_epochs * len(train_loader)\n",
    "warmup_steps = stage1_warmup_epochs * len(train_loader)\n",
    "scheduler = get_cosine_scheduler_with_warmup(optimizer, warmup_steps, total_steps, stage1_min_lr)\n",
    "\n",
    "# Early stopping for stage 1\n",
    "early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "# Stage 1 training loop\n",
    "global_step = 0\n",
    "for epoch in range(stage1_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{stage1_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, scaler, use_amp, eval_bn=eval_bn, gradient_clip_norm=gradient_clip_norm)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_preds, val_labels = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('Stage1/Train_Loss', train_loss, epoch)\n",
    "    writer.add_scalar('Stage1/Train_Acc', train_acc, epoch)\n",
    "    writer.add_scalar('Stage1/Val_Loss', val_loss, epoch)\n",
    "    writer.add_scalar('Stage1/Val_Acc', val_acc, epoch)\n",
    "    writer.add_scalar('Stage1/Learning_Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(val_loss, model):\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    global_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf61303",
   "metadata": {},
   "source": [
    "## STAGE 2: Fine-tuning entire network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d415e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze all parameters and fine-tune\n",
    "model = unfreeze_all(model)\n",
    "\n",
    "# Create new data loaders with smaller batch size for stage 2\n",
    "train_loader_stage2 = DataLoader(train_dataset, batch_size=stage2_batch_size, shuffle=True)\n",
    "val_loader_stage2 = DataLoader(val_dataset, batch_size=stage2_batch_size)\n",
    "\n",
    "# New optimizer and scheduler for stage 2 with lower learning rate\n",
    "param_groups = set_weight_decay(model, weight_decay=weight_decay)\n",
    "optimizer = optim.AdamW(\n",
    "    param_groups, \n",
    "    lr=stage2_lr, \n",
    "    eps=optimizer_config['eps'], \n",
    "    betas=optimizer_config['betas']\n",
    ")\n",
    "\n",
    "# Cosine annealing scheduler with warmup for stage 2\n",
    "total_steps = stage2_epochs * len(train_loader_stage2)  \n",
    "warmup_steps = stage2_warmup_epochs * len(train_loader_stage2)\n",
    "scheduler = get_cosine_scheduler_with_warmup(optimizer, warmup_steps, total_steps, stage2_min_lr)\n",
    "\n",
    "# Reset early stopping for stage 2\n",
    "early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "# Stage 2 training loop\n",
    "for epoch in range(stage2_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{stage2_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader_stage2, criterion, optimizer, device, scaler, use_amp, eval_bn=eval_bn, gradient_clip_norm=gradient_clip_norm)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_preds, val_labels = validate_epoch(model, val_loader_stage2, criterion, device)\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('Stage2/Train_Loss', train_loss, global_step + epoch)\n",
    "    writer.add_scalar('Stage2/Train_Acc', train_acc, global_step + epoch)\n",
    "    writer.add_scalar('Stage2/Val_Loss', val_loss, global_step + epoch)\n",
    "    writer.add_scalar('Stage2/Val_Acc', val_acc, global_step + epoch)\n",
    "    writer.add_scalar('Stage2/Learning_Rate', optimizer.param_groups[0]['lr'], global_step + epoch)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(val_loss, model):\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model with metadata\n",
    "model_save_dict = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'num_classes': num_classes,\n",
    "    'class_labels': class_labels,\n",
    "    'img_size': img_size,\n",
    "    'pretrained_path': pretrained_path\n",
    "}\n",
    "\n",
    "checkpoint_savepath = os.path.join(output_dir, checkpoint_name)\n",
    "torch.save(model_save_dict, checkpoint_savepath)\n",
    "\n",
    "print(f\"\\nModel saved to {checkpoint_savepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2262fcbf",
   "metadata": {},
   "source": [
    "## EVALUATION ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_correct, test_total = 0, 0\n",
    "test_preds, test_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "        \n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Log final test accuracy\n",
    "writer.add_scalar('Final/Test_Acc', test_acc, 0)\n",
    "\n",
    "# Generate dynamic file names based on checkpoint_name\n",
    "checkpoint_base = os.path.splitext(checkpoint_name)[0]  # Remove .pth extension\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm_path = os.path.join(logs_dir, f'{checkpoint_base}_confusion_matrix.png')\n",
    "plot_confusion_matrix(test_labels, test_preds, class_labels, cm_path)\n",
    "print(f\"Confusion matrix saved to {cm_path}\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_preds, target_names=class_labels)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Save classification report\n",
    "report_path = os.path.join(logs_dir, f'{checkpoint_base}_classification_report.txt')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Also save the training information as metadata\n",
    "summary_info = {\n",
    "    'final_test_accuracy': float(test_acc),\n",
    "    'num_classes': num_classes,\n",
    "    'class_labels': class_labels,\n",
    "    'model_architecture': 'TinyViT',\n",
    "    'base_model': base_model,\n",
    "    'pretrained_checkpoint': pretrained_path,\n",
    "    'training_stages': {\n",
    "        'stage1_epochs': stage1_epochs,\n",
    "        'stage1_lr': stage1_lr,\n",
    "        'stage2_epochs': stage2_epochs,\n",
    "        'stage2_lr': stage2_lr\n",
    "    },\n",
    "    'image_size': img_size,\n",
    "    'stage1_batch_size': stage1_batch_size,\n",
    "    'stage2_batch_size': stage2_batch_size\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(output_dir, f'{checkpoint_base}_metadata.json')\n",
    "dump_dict(summary_info, summary_path)\n",
    "print(f\"Training summary saved to {summary_path}\")\n",
    "\n",
    "writer.close()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf63df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
